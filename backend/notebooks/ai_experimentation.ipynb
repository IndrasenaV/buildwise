{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI Experimentation Notebook\n",
                "\n",
                "This notebook contains the Python translation of the AI logic found in `langchainService.js` and `vectorService.js`. \n",
                "It allows for isolated testing of prompts, chains, and vector operations.\n",
                "\n",
                "## Setup\n",
                "Ensure you have the following installed:\n",
                "```bash\n",
                "pip install langchain langchain-openai python-dotenv requests pypdf tiktoken\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mFailed to start the Kernel. \n",
                        "\u001b[1;31merror: externally-managed-environment\n",
                        "\u001b[1;31m\n",
                        "\u001b[1;31m× This environment is externally managed\n",
                        "\u001b[1;31m╰─> To install Python packages system-wide, try brew install\n",
                        "\u001b[1;31m    xyz, where xyz is the package you are trying to\n",
                        "\u001b[1;31m    install.\n",
                        "\u001b[1;31m    \n",
                        "\u001b[1;31m    If you wish to install a Python library that isn't in Homebrew,\n",
                        "\u001b[1;31m    use a virtual environment:\n",
                        "\u001b[1;31m    \n",
                        "\u001b[1;31m    python3 -m venv path/to/venv\n",
                        "\u001b[1;31m    source path/to/venv/bin/activate\n",
                        "\u001b[1;31m    python3 -m pip install xyz\n",
                        "\u001b[1;31m    \n",
                        "\u001b[1;31m    If you wish to install a Python application that isn't in Homebrew,\n",
                        "\u001b[1;31m    it may be easiest to use 'pipx install xyz', which will manage a\n",
                        "\u001b[1;31m    virtual environment for you. You can install pipx with\n",
                        "\u001b[1;31m    \n",
                        "\u001b[1;31m    brew install pipx\n",
                        "\u001b[1;31m    \n",
                        "\u001b[1;31m    You may restore the old behavior of pip by passing\n",
                        "\u001b[1;31m    the '--break-system-packages' flag to pip, or by adding\n",
                        "\u001b[1;31m    'break-system-packages = true' to your pip.conf file. The latter\n",
                        "\u001b[1;31m    will permanently disable this error.\n",
                        "\u001b[1;31m    \n",
                        "\u001b[1;31m    If you disable this error, we STRONGLY recommend that you additionally\n",
                        "\u001b[1;31m    pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
                        "\u001b[1;31m    file. Failure to do this can result in a broken Homebrew installation.\n",
                        "\u001b[1;31m    \n",
                        "\u001b[1;31m    Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
                        "\u001b[1;31m\n",
                        "\u001b[1;31mnote: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
                        "\u001b[1;31mhint: See PEP 668 for the detailed specification. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import requests\n",
                "import time\n",
                "import json\n",
                "import re\n",
                "from typing import List, Optional, Dict, Any\n",
                "from pathlib import Path\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# LangChain Imports\n",
                "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
                "from langchain_core.messages import HumanMessage, SystemMessage\n",
                "from langchain_core.output_parsers import JsonOutputParser"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Environment Variables\n",
                "# Assuming the notebook is in backend/notebooks/, we look for .env in backend/ or root\n",
                "env_path = Path(\"../.env\")\n",
                "if not env_path.exists():\n",
                "    env_path = Path(\"../../.env\")\n",
                "\n",
                "load_dotenv(dotenv_path=env_path)\n",
                "\n",
                "if not os.getenv(\"OPENAI_API_KEY\"):\n",
                "    print(\"WARNING: OPENAI_API_KEY not found in environment\")\n",
                "else:\n",
                "    print(\"Environment loaded successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. LlamaParse Integration\n",
                "Manual implementation of LlamaCloud PDF parsing to match `langchainService.js`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_pdf_with_llama(source: str | bytes) -> str:\n",
                "    api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
                "    if not api_key:\n",
                "        raise ValueError(\"Missing LLAMA_CLOUD_API_KEY\")\n",
                "\n",
                "    # 1. Get File Content\n",
                "    file_content = None\n",
                "    filename = \"upload.pdf\"\n",
                "\n",
                "    if isinstance(source, bytes):\n",
                "        file_content = source\n",
                "    elif isinstance(source, str):\n",
                "        if source.startswith(\"http\"):\n",
                "            res = requests.get(source)\n",
                "            res.raise_for_status()\n",
                "            file_content = res.content\n",
                "        else:\n",
                "            with open(source, \"rb\") as f:\n",
                "                file_content = f.read()\n",
                "            filename = os.path.basename(source)\n",
                "\n",
                "    # 2. Upload File\n",
                "    base_url = \"https://api.cloud.llamaindex.ai/api/parsing\"\n",
                "    headers = {\n",
                "        \"Authorization\": f\"Bearer {api_key}\"\n",
                "    }\n",
                "    files = {\n",
                "        \"file\": (filename, file_content, \"application/pdf\")\n",
                "    }\n",
                "\n",
                "    upload_res = requests.post(f\"{base_url}/upload\", headers=headers, files=files)\n",
                "    upload_res.raise_for_status()\n",
                "    \n",
                "    job_id = upload_res.json().get(\"id\")\n",
                "    print(f\"[LlamaParse] Job started: {job_id}\")\n",
                "\n",
                "    # 3. Poll for result\n",
                "    max_retries = 300\n",
                "    for _ in range(max_retries):\n",
                "        time.sleep(1)\n",
                "        job_res = requests.get(f\"{base_url}/job/{job_id}\", headers=headers)\n",
                "        if not job_res.ok:\n",
                "            continue\n",
                "        \n",
                "        job_data = job_res.json()\n",
                "        status = job_data.get(\"status\")\n",
                "\n",
                "        if status == \"SUCCESS\":\n",
                "            result_res = requests.get(f\"{base_url}/job/{job_id}/result/markdown\", headers=headers)\n",
                "            result_res.raise_for_status()\n",
                "            data = result_res.json()\n",
                "            return data.get(\"markdown\", str(data))\n",
                "        elif status == \"FAILED\":\n",
                "            raise Exception(f\"LlamaParse Job Failed: {job_data}\")\n",
                "    \n",
                "    raise Exception(\"LlamaParse Timed Out\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. LangChain Service\n",
                "Core LLM execution logic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_langchain_llm(model: str = 'gpt-4o-mini', temperature: float = 0.2) -> ChatOpenAI:\n",
                "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
                "    if not api_key:\n",
                "        raise ValueError(\"Missing OPENAI_API_KEY\")\n",
                "    return ChatOpenAI(\n",
                "        model_name=model,\n",
                "        temperature=temperature,\n",
                "        openai_api_key=api_key\n",
                "    )\n",
                "\n",
                "def extract_text_from_url(url: str) -> str:\n",
                "    # Simplified for notebook: supports basic text fetching. \n",
                "    # For PDFs in list, logic usually routes to pdf extractors.\n",
                "    try:\n",
                "        res = requests.get(url)\n",
                "        res.raise_for_status()\n",
                "        content_type = res.headers.get('Content-Type', '')\n",
                "        \n",
                "        if 'application/pdf' in content_type or url.lower().endswith('.pdf'):\n",
                "            # Start LlamaParse or use simple pypdf if needed\n",
                "            return \"[PDF Content Placeholder - Use parse_pdf_with_llama for full extraction]\"\n",
                "        \n",
                "        return res.text\n",
                "    except Exception as e:\n",
                "        return f\"Error fetching {url}: {str(e)}\"\n",
                "\n",
                "async def execute_with_langchain(\n",
                "    system_prompt: str,\n",
                "    user_prompt: str,\n",
                "    urls: List[str] = [],\n",
                "    model: str = 'gpt-4o-mini',\n",
                "    temperature: float = 0.2,\n",
                "    supports_images: bool = False,\n",
                "    extra_context: str = '',\n",
                "    schema: Optional[Dict] = None\n",
                ") -> Dict[str, Any]:\n",
                "    \n",
                "    image_extensions = ('.png', '.jpg', '.jpeg', '.webp', '.gif')\n",
                "    pdf_urls = [u for u in urls if '.pdf' in u.lower()]\n",
                "    image_urls = [u for u in urls if u.lower().endswith(image_extensions)]\n",
                "    text_urls = [u for u in urls if u not in pdf_urls and u not in image_urls]\n",
                "\n",
                "    combined_text = \"\"\n",
                "\n",
                "    # Process Text URLs\n",
                "    for i, url in enumerate(text_urls):\n",
                "        text = extract_text_from_url(url)\n",
                "        combined_text += f\"\\n\\n--- Document {i+1}: {url} ---\\n{text}\"\n",
                "\n",
                "    # Process PDFs\n",
                "    for i, url in enumerate(pdf_urls):\n",
                "        try:\n",
                "            # Note: In real app, we usually cache this or use lightweight PDF parser for speed if LlamaParse is too slow\n",
                "            # Here we just use a placeholder text or call parse_pdf_with_llama if you want to wait\n",
                "            # text = parse_pdf_with_llama(url) \n",
                "            text = \"[PDF Content - Enable LlamaParse call in notebook to fetch]\"\n",
                "            combined_text += f\"\\n\\n--- PDF {i+1}: {url} ---\\n{text}\"\n",
                "        except Exception as e:\n",
                "             combined_text += f\"\\n\\n--- PDF {i+1}: {url} (Error: {e}) ---\\n\"\n",
                "\n",
                "    if extra_context:\n",
                "        combined_text += f\"\\n\\n--- Additional Context ---\\n{extra_context[:30000]}\"\n",
                "\n",
                "    # Build Messages\n",
                "    messages = [SystemMessage(content=system_prompt)]\n",
                "    \n",
                "    user_content_parts = [user_prompt]\n",
                "    if combined_text.strip():\n",
                "        user_content_parts.append(f\"Relevant document text:\\n{combined_text[:200000]}\")\n",
                "\n",
                "    llm = None\n",
                "    if supports_images and image_urls:\n",
                "        vision_model = model if 'gpt-4o' in model else 'gpt-4o'\n",
                "        llm = get_langchain_llm(vision_model, temperature)\n",
                "        \n",
                "        content_block = [\n",
                "            {\"type\": \"text\", \"text\": \"\\n\\n\".join(user_content_parts)}\n",
                "        ]\n",
                "        for url in image_urls[:10]:\n",
                "            content_block.append({\n",
                "                \"type\": \"image_url\",\n",
                "                \"image_url\": {\"url\": url}\n",
                "            })\n",
                "        messages.append(HumanMessage(content=content_block))\n",
                "    else:\n",
                "        llm = get_langchain_llm(model, temperature)\n",
                "        messages.append(HumanMessage(content=\"\\n\\n\".join(user_content_parts)))\n",
                "\n",
                "    # Execute\n",
                "    if schema:\n",
                "        structured_llm = llm.with_structured_output(schema)\n",
                "        result = structured_llm.invoke(messages)\n",
                "        # Result is already a dict usually with structured output\n",
                "        return {\"data\": result}\n",
                "    else:\n",
                "        response = llm.invoke(messages)\n",
                "        return {\n",
                "            \"text\": response.content,\n",
                "            \"usage\": response.response_metadata.get(\"usage\")\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Vector Service (In-Memory Mock)\n",
                "Mocking the MongoDB Vector Store with a simple in-memory list."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class InMemoryVectorStore:\n",
                "    def __init__(self):\n",
                "        self.store = [] # List of { embedding, content, metadata }\n",
                "\n",
                "    def add(self, embedding, content, metadata):\n",
                "        self.store.append({\n",
                "            \"embedding\": embedding,\n",
                "            \"content\": content,\n",
                "            \"metadata\": metadata\n",
                "        })\n",
                "\n",
                "    def search(self, query_vector, limit=5):\n",
                "        # Simple Cosine Similarity\n",
                "        import numpy as np\n",
                "        \n",
                "        if not self.store:\n",
                "            return []\n",
                "            \n",
                "        q = np.array(query_vector)\n",
                "        results = []\n",
                "\n",
                "        for item in self.store:\n",
                "            v = np.array(item[\"embedding\"])\n",
                "            # Cosine Sim: (A . B) / (||A|| * ||B||)\n",
                "            score = np.dot(q, v) / (np.linalg.norm(q) * np.linalg.norm(v))\n",
                "            results.append({ **item, \"score\": score })\n",
                "        \n",
                "        return sorted(results, key=lambda x: x[\"score\"], reverse=True)[:limit]\n",
                "\n",
                "# Global Store Instance\n",
                "vector_store = InMemoryVectorStore()\n",
                "\n",
                "def get_embedding(text: str) -> List[float]:\n",
                "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
                "    embeddings = OpenAIEmbeddings(\n",
                "        openai_api_key=api_key,\n",
                "        model=\"text-embedding-3-small\"\n",
                "    )\n",
                "    return embeddings.embed_query(text)\n",
                "\n",
                "async def ingest_text(text: str, home_id: str, meta: Dict = {}):\n",
                "    if not text or not text.strip(): return 0\n",
                "    \n",
                "    # Simple splitter (SentenceSplitter not standard in Py LangChain, using RecursiveCharacterTextSplitter)\n",
                "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "    chunks = splitter.split_text(text)\n",
                "\n",
                "    print(f\"Split into {len(chunks)} chunks...\")\n",
                "    \n",
                "    for chunk in chunks:\n",
                "        emb = get_embedding(chunk)\n",
                "        vector_store.add(emb, chunk, { \"homeId\": home_id, **meta })\n",
                "    \n",
                "    return len(chunks)\n",
                "\n",
                "async def search_similar(query: str, home_id: str, limit: int = 5):\n",
                "    if not query:\n",
                "        return []\n",
                "    \n",
                "    query_emb = get_embedding(query)\n",
                "    results = vector_store.search(query_emb, limit)\n",
                "    # Filter by home_id if needed, but in mock we might just show all or filter post-search\n",
                "    return [r for r in results if r[\"metadata\"].get(\"homeId\") == home_id]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Playground\n",
                "Use the cells below to experiment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Chat\n",
                "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
                "USER_PROMPT = \"Explain the benefits of vector databases briefly.\"\n",
                "\n",
                "# Need to run this in async context or use await in top-level notebook\n",
                "result = await execute_with_langchain(SYSTEM_PROMPT, USER_PROMPT)\n",
                "print(result['text'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Ingest & Search\n",
                "sample_text = \"\"\"\n",
                "Buildwise is an AI-powered home building platform.\n",
                "It helps manage permits, designs, and contractors.\n",
                "Buildwise uses MongoDB and LangChain.\n",
                "\"\"\"\n",
                "\n",
                "await ingest_text(sample_text, home_id=\"test-home-1\")\n",
                "\n",
                "search_res = await search_similar(\"What tech stack does Buildwise use?\", home_id=\"test-home-1\")\n",
                "for r in search_res:\n",
                "    print(f\"Score: {r['score']:.4f} | Content: {r['content']}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
